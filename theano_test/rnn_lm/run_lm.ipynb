{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import operator\n",
    "import io\n",
    "import argparse\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from datetime import datetime\n",
    "from gru import GRUTheano\n",
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START_TOKEN = \"<bos>\"\n",
    "SENTENCE_END_TOKEN = \"<eos>\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_word_freq(sentences):\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 0\n",
    "            word_dict[word] += 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename=\"data/select_top_3000_cn\", vocabulary_size=5000, min_sent_characters=0):\n",
    "\n",
    "    word_to_index = []\n",
    "    index_to_word = []\n",
    "\n",
    "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "    print(\"Reading data file...\")\n",
    "    with open(filename, 'r') as f:\n",
    "        sentences = [s.strip() for s in f if len(s) > min_sent_characters]\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (SENTENCE_START_TOKEN, x, SENTENCE_END_TOKEN) for x in sentences]\n",
    "        sentences = [sent.split(\" \") for sent in sentences]\n",
    "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "    if not os.path.isfile(filename + \".vocab.pkl\"):\n",
    "        # Count the word frequencies\n",
    "        word_freq = get_word_freq(sentences)\n",
    "        print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "        # Get the most common words and build index_to_word and word_to_index vectors\n",
    "        vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)[:vocabulary_size-2]\n",
    "        cPickle.dump(vocab, open(filename + \".vocab.pkl\", \"w\"))\n",
    "    else:\n",
    "        vocab = cPickle.load(open(filename + \".vocab.pkl\", \"r\"))\n",
    "    print(\"Using vocabulary size %d.\" % len(vocab))\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "    sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "    index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN] + [x[0] for x in sorted_vocab]\n",
    "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]\n",
    "\n",
    "    # Create the training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in sentences])\n",
    "\n",
    "    return X_train, y_train, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_multinomial(p):\n",
    "    \"\"\"\n",
    "       Sample multinomial distribution with parameters given by p\n",
    "       Returns an int    \n",
    "    \"\"\"\n",
    "    x = np.random.uniform(0, 1)\n",
    "    for i,v in enumerate(np.cumsum(p)):\n",
    "        if x < v: return i\n",
    "    return len(p) - 1 # shouldn't happen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model, index_to_word, word_to_index, min_length=5):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[SENTENCE_START_TOKEN]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[SENTENCE_END_TOKEN]:\n",
    "        next_word_probs = model.predict(new_sentence)[-1]\n",
    "        sampled_word = sample_multinomial(next_word_probs)\n",
    "        #samples = np.random.multinomial(1, next_word_probs)\n",
    "        #sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        if len(new_sentence) > 100 or sampled_word == word_to_index[UNKNOWN_TOKEN]:\n",
    "            return None\n",
    "    if len(new_sentence) < min_length:\n",
    "        return None\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_sentences(model, n, index_to_word, word_to_index):\n",
    "    for i in range(n):\n",
    "        sent = None\n",
    "        while not sent:\n",
    "            sent = generate_sentence(model, index_to_word, word_to_index)\n",
    "        sentence_str = [index_to_word[x] for x in sent[1:-1]]\n",
    "        print(\" \".join(sentence_str))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    x_train, y_train, word_to_index, index_to_word = load_data(\"data/select_top_3000_cn\", 5000)\n",
    "    print \"building model...\"\n",
    "    model = GRUTheano(len(index_to_word), hidden_dim=256, bptt_truncate=-1)\n",
    "    if False:\n",
    "        model.load_params(\"first.model\")\n",
    "    print \"building f_grad and f_update...\"\n",
    "    lr = T.scalar(name='lr')\n",
    "    f_grad, f_update = sgd(lr, model.params, model.grads, [model.x, model.y], model.cost)\n",
    "    print \"building model done.\"\n",
    "    # Print SGD step time\n",
    "    t1 = time.time()\n",
    "    cost = f_grad(x_train[10], y_train[10])\n",
    "    f_update(0.001)\n",
    "    t2 = time.time()\n",
    "    print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        num_examples_seen = 0\n",
    "        for i in np.random.permutation(len(y_train)):\n",
    "            # One SGD step\n",
    "            cost = f_grad(x_train[i], y_train[i])\n",
    "            if np.isnan(cost):\n",
    "                print 'NaN detected: ', i \n",
    "                continue\n",
    "            if np.isinf(cost):\n",
    "                print 'inf detected: ', i \n",
    "                continue \n",
    "            f_update(0.001)\n",
    "            num_examples_seen += 1\n",
    "            # Optionally do callback\n",
    "            if (25 and num_examples_seen % 25 == 0):\n",
    "                print (\"epoch %d, num_examples_seen %d\" % (epoch,  num_examples_seen))\n",
    "                dt = datetime.now().isoformat()\n",
    "                loss = model.calculate_loss(x_train[:10000], y_train[:10000])\n",
    "                print(\"\\n%s [loss]: %f\" % (dt, loss))\n",
    "                model.save_params(\"first.model\")\n",
    "                print (\"model saved in %s done.\" % \"first.model\")\n",
    "                print(\"--------------------------------------------------\")\n",
    "                generate_sentences(model, 10, index_to_word, word_to_index)\n",
    "                print(\"\\n\")\n",
    "                sys.stdout.flush()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data file...\n",
      "Parsed 670654 sentences.\n",
      "Using vocabulary size 3000.\n",
      "The least frequent word in our vocabulary is '伊斯兰' and appeared 1 times.\n",
      "building model...\n",
      "compile grads...\n",
      "building f_grad and f_update...\n",
      "building model done.\n",
      "SGD Step time: 138.652086 milliseconds\n",
      "epoch 0, num_examples_seen 25\n",
      "\n",
      "2017-03-11T12:43:06.819937 [loss]: 8.002977\n",
      "model saved in first.model done.\n",
      "--------------------------------------------------\n",
      "学家 太阳能 想到 太阳能 装 几 作者 25 支出 达 工具 到底 当中 当地 等等 信号 查询 机关 哈 谷歌 区别 群体 货物 科学 保留 亿 美好 相应 有着 卫星 较 作 拥有 15 最高 世 著 钱 定期 巴勒斯坦 按照 观看 很高 蒂 信号 论坛 有限 运动员 23 不安 学校 富 不在 它们 股票 里面 您 总结 此时 律师 政府 大概 返回 边界 跟 应用 集 专家 信息 孩子 销售 航空 场 字 同性恋 研究者 ” 优化 二氧化碳 之 们 最好 年 鼓励\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-70d2f232c25c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"model saved in %s done.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m\"first.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mgenerate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-bf067467a058>\u001b[0m in \u001b[0;36mgenerate_sentences\u001b[0;34m(model, n, index_to_word, word_to_index)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msentence_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-81f208fe36b8>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(model, index_to_word, word_to_index, min_length)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Repeat until we get an end token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSENTENCE_END_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mnext_word_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msampled_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_multinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#samples = np.random.multinomial(1, next_word_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/majing/.theano/compiledir_Darwin-16.0.0-x86_64-i386-64bit-i386-2.7.12-64/scan_perform/mod.cpp:6489)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mvalue_zeros\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \"\"\"\n\u001b[1;32m    631\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
