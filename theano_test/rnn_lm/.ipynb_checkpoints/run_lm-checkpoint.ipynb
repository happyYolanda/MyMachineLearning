{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import operator\n",
    "import io\n",
    "import argparse\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from datetime import datetime\n",
    "from gru import GRUTheano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START_TOKEN = \"<bos>\"\n",
    "SENTENCE_END_TOKEN = \"<eos>\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_freq(sentences):\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 0\n",
    "            word_dict[word] += 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename=\"data/select_top_3000_cn\", vocabulary_size=5000, min_sent_characters=0):\n",
    "\n",
    "    word_to_index = []\n",
    "    index_to_word = []\n",
    "\n",
    "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "    print(\"Reading data file...\")\n",
    "    with open(filename, 'r') as f:\n",
    "        sentences = [s.strip() for s in f if len(s) > min_sent_characters]\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (SENTENCE_START_TOKEN, x, SENTENCE_END_TOKEN) for x in sentences]\n",
    "        sentences = [sent.split(\" \") for sent in sentences]\n",
    "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "    if not os.path.isfile(filename + \".vocab.pkl\"):\n",
    "        # Count the word frequencies\n",
    "        word_freq = get_word_freq(sentences)\n",
    "        print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "        # Get the most common words and build index_to_word and word_to_index vectors\n",
    "        vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)[:vocabulary_size-2]\n",
    "        cPickle.dump(vocab, open(filename + \".vocab.pkl\", \"w\"))\n",
    "    else:\n",
    "        vocab = cPickle.load(open(filename + \".vocab.pkl\", \"r\"))\n",
    "    print(\"Using vocabulary size %d.\" % len(vocab))\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "    sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "    index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN] + [x[0] for x in sorted_vocab]\n",
    "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]\n",
    "\n",
    "    # Create the training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in sentences])\n",
    "\n",
    "    return X_train, y_train, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_multinomial(p):\n",
    "    \"\"\"\n",
    "       Sample multinomial distribution with parameters given by p\n",
    "       Returns an int    \n",
    "    \"\"\"\n",
    "    x = np.random.uniform(0, 1)\n",
    "    for i,v in enumerate(np.cumsum(p)):\n",
    "        if x < v: return i\n",
    "    return len(p) - 1 # shouldn't happen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model, index_to_word, word_to_index, min_length=5):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[SENTENCE_START_TOKEN]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[SENTENCE_END_TOKEN]:\n",
    "        next_word_probs = model.predict(new_sentence)[-1]\n",
    "        sampled_word = sample_multinomial(next_word_probs)\n",
    "        #samples = np.random.multinomial(1, next_word_probs)\n",
    "        #sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        if len(new_sentence) > 100 or sampled_word == word_to_index[UNKNOWN_TOKEN]:\n",
    "            return None\n",
    "    if len(new_sentence) < min_length:\n",
    "        return None\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentences(model, n, index_to_word, word_to_index):\n",
    "    for i in range(n):\n",
    "        sent = None\n",
    "        while not sent:\n",
    "            sent = generate_sentence(model, index_to_word, word_to_index)\n",
    "        sentence_str = [index_to_word[x] for x in sent[1:-1]]\n",
    "        print(\" \".join(sentence_str))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Load data\n",
    "    x_train, y_train, word_to_index, index_to_word = load_data(args.dataset, args.vocab_size)\n",
    "    print \"building model...\"\n",
    "    model = GRUTheano(len(index_to_word), hidden_dim=args.hidden_dim, bptt_truncate=-1)\n",
    "    if args.reload:\n",
    "        model.load_params(args.model)\n",
    "    print \"building f_grad and f_update...\"\n",
    "    lr = T.scalar(name='lr')\n",
    "    f_grad, f_update = sgd(lr, model.params, model.grads, [model.x, model.y], model.cost)\n",
    "    print \"building model done.\"\n",
    "    # Print SGD step time\n",
    "    t1 = time.time()\n",
    "    cost = f_grad(x_train[10], y_train[10])\n",
    "    f_update(args.learning_rate)\n",
    "    t2 = time.time()\n",
    "    print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        num_examples_seen = 0\n",
    "        for i in np.random.permutation(len(y_train)):\n",
    "            # One SGD step\n",
    "            cost = f_grad(x_train[i], y_train[i])\n",
    "            if np.isnan(cost):\n",
    "                print 'NaN detected: ', i \n",
    "                continue\n",
    "            if np.isinf(cost):\n",
    "                print 'inf detected: ', i \n",
    "                continue \n",
    "            f_update(args.learning_rate)\n",
    "            num_examples_seen += 1\n",
    "            # Optionally do callback\n",
    "            if (args.print_every and num_examples_seen % args.print_every == 0):\n",
    "                print (\"epoch %d, num_examples_seen %d\" % (epoch,  num_examples_seen))\n",
    "                dt = datetime.now().isoformat()\n",
    "                loss = model.calculate_loss(x_train[:10000], y_train[:10000])\n",
    "                print(\"\\n%s [loss]: %f\" % (dt, loss))\n",
    "                model.save_params(args.model)\n",
    "                print (\"model saved in %s done.\" % args.model)\n",
    "                print(\"--------------------------------------------------\")\n",
    "                generate_sentences(model, 10, index_to_word, word_to_index)\n",
    "                print(\"\\n\")\n",
    "                sys.stdout.flush()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"Sample language model\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--vocab-size\", type=int, default=5000)\n",
    "    parser.add_argument(\"--hidden-dim\", type=int, default=256)\n",
    "    parser.add_argument(\"--embedding-dim\", type=int, default=100)\n",
    "    parser.add_argument(\"--print-every\", type=int, default=25000)\n",
    "    parser.add_argument(\"--model\", help=\"file to store the model\")\n",
    "    parser.add_argument(\"--dataset\", help=\"input training data file\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\", default=False, help=\"True for test, False for train\")\n",
    "    parser.add_argument(\"--reload\", action=\"store_true\", default=False, help=\"True reload model\")\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "    args = {\"test, true\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-36cd2df5a9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".vocab.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".vocab.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0msorted_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mindex_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<MASK/>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNKNOWN_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "if args.test:\n",
    "        if os.path.isfile(args.dataset + \".vocab.pkl\"):\n",
    "            vocab = cPickle.load(open(args.dataset + \".vocab.pkl\", \"r\"))\n",
    "            sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "            index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN] + [x[0] for x in sorted_vocab]\n",
    "            word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "            print \"building model...\"\n",
    "            model = GRUTheano(len(index_to_word), hidden_dim=args.hidden_dim, bptt_truncate=-1)\n",
    "            print \"build model done.\"\n",
    "            model.load_params(args.model)\n",
    "            generate_sentences(model, 10, index_to_word, word_to_index)\n",
    "        else:\n",
    "            print \"[Error]: vocab file is not avaliable!\"\n",
    "else:\n",
    "        train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
